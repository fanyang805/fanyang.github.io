---
layout: post
title: Deep Q Learning 
date: 2021-04-29
description: A summary of Deep Q network
---
Reinforcement learning is unstable and even diverge when a nonlinear function approximator such as neural network is used to represent the action-value (Q function). The causes are the correlations present in the sequence of observations. Small updates to Q may significantly change the policy and change the data distribution, the correlations between action-value (Q) and target value.

Two key ideas are used to address the correlation:  
(1) Replay buffer.  
(2) Update the target value periodically instead of every iteration.

Deep Q learning is (1) model free: not explicitly estimate the reward and transition dynamics. (2) off-policy: learn the greedy policy but execute action based on eps-greedy behaviour policy for exploration.

Advantage of replay buffer: (1) each step of experience is potentially used in many updates, which allows for greater data efficiency. (2) break the correlation between samples. (3) The behaviour distribution is average over many of its previous states smoothing out  learning and aoviding oscillations or divergence in the parameters. 
